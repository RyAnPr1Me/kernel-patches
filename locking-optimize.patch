From h9f8e7d6c5b4f3e2d9c1a8b7e6f5d4c9a3e8b7d6 Mon Sep 17 00:00:00 2001
From: Performance Patches <patches@kernel-perf.dev>
Date: Mon, 20 Jan 2025 17:20:00 +0000
Subject: [PATCH] locking: Optimize spinlocks for Zen 4

Optimize kernel locking for Zen 4's fast cache coherency:
- Reduced spin delays (faster MESI protocol)
- Better qspinlock tuning for high core counts
- Adaptive spinning based on CCX topology

Zen 4 benefits:
- Lower lock contention overhead
- Better scaling on 16+ core systems
- Reduced cache line bouncing between CCXs
- Improved multi-threaded workload performance

Signed-off-by: Performance Patches <patches@kernel-perf.dev>
---
 kernel/locking/qspinlock.c | 32 +++++++++++++++++++++++++++++++-
 kernel/locking/rwsem.c     | 18 +++++++++++++++++-
 2 files changed, 48 insertions(+), 2 deletions(-)

diff --git a/kernel/locking/qspinlock.c b/kernel/locking/qspinlock.c
index 65a9a37bb530..f8c9d4e2a1b3 100644
--- a/kernel/locking/qspinlock.c
+++ b/kernel/locking/qspinlock.c
@@ -30,7 +30,15 @@
  * On x86 the ASM variant is preferred, but on other architectures, the
  * generic implementation may perform better.
  */
-#define _Q_PENDING_LOOPS(1 << 9)
+/*
+ * Zen 4 optimization: Reduce spin loops due to faster cache coherency
+ * Zen 4's MESI protocol is much faster than older CPUs
+ */
+#ifdef CONFIG_X86
+#define _Q_PENDING_LOOPS(1 << 7)  /* 128 instead of 512 */
+#else
+#define _Q_PENDING_LOOPS(1 << 9)  /* Keep default for other archs */
+#endif
 
 #ifndef _Q_PENDING_LOOPS
 #define _Q_PENDING_LOOPS1
@@ -423,6 +431,28 @@ void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 if (new != _Q_LOCKED_VAL)
 goto queue;
 
+/*
+ * Zen 4 fast path: Try adaptive spinning based on lock holder location
+ * If lock is held by same-CCX CPU, spin longer (low latency)
+ * If lock is held by different-CCX CPU, queue sooner (high latency)
+ */
+#ifdef CONFIG_NUMA
+if (numa_mode) {
+int holder_cpu = (val >> _Q_LOCKED_CPU_SHIFT) & _Q_LOCKED_CPU_MASK;
+int current_cpu = raw_smp_processor_id();
+
+/* Check if on same CCX (L3 cache domain) */
+if (cpu_to_node(holder_cpu) == cpu_to_node(current_cpu)) {
+/* Same CCX: spin longer (intra-CCX ~40ns) */
+for (int i = 0; i < _Q_PENDING_LOOPS * 2; i++) {
+if (atomic_read(&lock->val) == _Q_LOCKED_VAL)
+cpu_relax();
+else
+goto locked;
+}
+}
+}
+#endif
 /*
  * We're pending, wait for the owner to go away.
  *
diff --git a/kernel/locking/rwsem.c b/kernel/locking/rwsem.c
index e63f740c2cc8..f8c9d4e2a1b3 100644
--- a/kernel/locking/rwsem.c
+++ b/kernel/locking/rwsem.c
@@ -680,7 +680,23 @@ rwsem_spin_on_owner(struct rw_semaphore *sem)
  * Use smp_cond_load_relaxed() to avoid usage of smp_rmb().
  */
 if (owner_on_cpu(owner)) {
-smp_cond_load_relaxed(&sem->owner,
+/*
+ * Zen 4: Shorter spins for cross-CCX lock holders
+ * Longer spins for same-CCX (better cache locality)
+ */
+int spin_limit = RWSEM_SPINNING_TIMEOUT;
+#ifdef CONFIG_NUMA
+if (owner && owner->on_cpu) {
+int owner_cpu = task_cpu(owner);
+int current_cpu = raw_smp_processor_id();
+
+/* Same CCX: spin 2x longer */
+if (cpu_to_node(owner_cpu) == cpu_to_node(current_cpu))
+spin_limit *= 2;
+}
+#endif
+
+smp_cond_load_relaxed_timeout(&sem->owner, spin_limit,
       !owner || !owner_on_cpu(owner));
 }
 
-- 
2.43.0
