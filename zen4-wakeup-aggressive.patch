Subject: [PATCH] sched/fair: Aggressive Zen 4 CCX-aware wakeup optimization

Aggressively optimize task wakeup for AMD Zen 4 (Ryzen 7000/EPYC Genoa):
- Prefer same-CCX idle CPUs to minimize L3 cache misses
- Reduce cross-CCD migrations (40ns vs 100ns+ latency)
- Optimize for gaming workloads on high-core-count systems

Zen 4 topology:
- 8 cores per CCX (Core Complex)
- Up to 2 CCX per CCD (Core Chiplet Die)  
- 32MB L3 cache shared per CCX
- ~40ns intra-CCX latency, ~100ns+ inter-CCX latency

For personal gaming systems with Zen 4 + NVIDIA GPUs.
NOT intended for upstream submission.

Signed-off-by: Zen4-Optimizer <zen4@personal-kernel.local>
---
 kernel/sched/fair.c | 45 +++++++++++++++++++++++++++++++++++++++++++++
 1 file changed, 45 insertions(+)

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 000000000000..111111111111 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -7100,6 +7100,47 @@ static int find_energy_efficient_cpu(struct task_struct *p, int prev_cpu)
 return target;
 }
 
+/*
+ * Zen 4 CCX-aware idle CPU selection
+ * 
+ * Aggressively prefer CPUs in the same L3 cache domain (CCX) to minimize
+ * cache misses. Zen 4 has 32MB L3 per CCX shared among 8 cores.
+ *
+ * Latency impact:
+ * - Same CCX: ~40ns
+ * - Different CCX, same CCD: ~80-100ns  
+ * - Different CCD: ~100ns+
+ */
+static int zen4_select_idle_sibling_fast(struct task_struct *p, int prev_cpu, int target)
+{
+const struct cpumask *llc_mask;
+int cpu;
+
+/* Get L3 cache domain (CCX) mask */
+llc_mask = cpu_llc_shared_mask(prev_cpu);
+if (!llc_mask)
+return -1;
+
+/* Fast path: Try previous CPU first if idle */
+if (available_idle_cpu(prev_cpu) && cpumask_test_cpu(prev_cpu, p->cpus_ptr))
+return prev_cpu;
+
+/* Search same CCX for idle CPU */
+for_each_cpu(cpu, llc_mask) {
+if (cpu == prev_cpu)
+continue;
+if (available_idle_cpu(cpu) && cpumask_test_cpu(cpu, p->cpus_ptr))
+return cpu;
+}
+
+/* No idle CPU in same CCX - let default logic handle it */
+return -1;
+}
+
+/* Check if we're on AMD Zen 4 (Family 19h, Model 0x10-0x7F) */
+static inline bool is_zen4_cpu(void)
+{
+return boot_cpu_data.x86_vendor == X86_VENDOR_AMD && 
+       boot_cpu_data.x86 == 0x19 && 
+       boot_cpu_data.x86_model >= 0x10 && boot_cpu_data.x86_model <= 0x7F;
+}
+
 /*
  * select_task_rq_fair: Select target runqueue for the waking task in domains
  * that have the relevant SD flag set. In practice, this is SD_BALANCE_WAKE,
@@ -7120,6 +7161,10 @@ select_task_rq_fair(struct task_struct *p, int prev_cpu, int wake_flags)
 
 rcu_read_lock();
 for_each_domain(cpu, tmp) {
+/* Zen 4 fast path: Try CCX-local selection first */
+if (is_zen4_cpu() && (wake_flags & WF_TTWU)) {
+int zen4_cpu = zen4_select_idle_sibling_fast(p, prev_cpu, cpu);
+if (zen4_cpu >= 0) {
+new_cpu = zen4_cpu;
+goto unlock;
+}
+}
+
 /*
  * If both 'cpu' and 'prev_cpu' are part of this domain,
  * cpu is a valid SD_WAKE_AFFINE target.
