From 8e9f7d6c5a3b4f2e9d1c6a8b5e7d3f9c4a6e8b2d Mon Sep 17 00:00:00 2001
From: Performance Patches <patches@kernel-perf.dev>
Date: Mon, 20 Jan 2025 15:40:00 +0000
Subject: [PATCH] block: Implement adaptive readahead for NVMe

Implement adaptive block layer readahead for modern NVMe SSDs:
- Detect sequential vs random I/O patterns
- Scale readahead with device capabilities
- Zen 4 PCIe 5.0 aware tuning

Tunables:
- block_ra_multiplier: Readahead scaling factor (default: 4)
- block_ra_max_kb: Maximum readahead size (default: 2048)

Zen 4 benefits:
- Exploits PCIe 5.0 bandwidth (up to 14GB/s)
- Better NVMe queue utilization
- Reduced latency for sequential reads
- Improved game level loading

Signed-off-by: Performance Patches <patches@kernel-perf.dev>
---
 block/blk-core.c     | 28 ++++++++++++++++++++++++++++
 block/blk-settings.c | 15 ++++++++++++++-
 2 files changed, 42 insertions(+), 1 deletion(-)

diff --git a/block/blk-core.c b/block/blk-core.c
index 9e5e0277a4d6..f8c9d4e2a1b3 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -54,6 +54,13 @@
 #include "blk-pm.h"
 #include "blk-cgroup.h"
 
+/*
+ * Zen 4 PCIe 5.0 optimized readahead
+ */
+static unsigned int block_ra_multiplier __read_mostly = 4;
+static unsigned int block_ra_max_kb __read_mostly = 2048;
+static unsigned int block_sequential_threshold __read_mostly = 8;
+
 struct dentry *blk_debugfs_root;
 
 EXPORT_TRACEPOINT_SYMBOL_GPL(block_bio_remap);
@@ -711,6 +718,27 @@ void submit_bio_noacct(struct bio *bio)
 goto end_io;
 }
 
+/*
+ * Zen 4 adaptive readahead: Detect sequential patterns and
+ * increase readahead for better PCIe 5.0 utilization
+ */
+if (op_is_read(bio_op(bio)) && bio->bi_iter.bi_size > 0) {
+struct request_queue *q = bdev_get_queue(bio->bi_bdev);
+sector_t sector = bio->bi_iter.bi_sector;
+
+/* Simple sequential detection */
+if (q && q->last_sector && 
+    sector == q->last_sector + (q->last_size >> 9)) {
+q->sequential_count++;
+
+/* Boost readahead on sequential workloads */
+if (q->sequential_count > block_sequential_threshold) {
+unsigned long ra_kb = min(block_ra_max_kb,
+q->backing_dev_info->ra_pages << (PAGE_SHIFT - 10) * block_ra_multiplier);
+q->backing_dev_info->ra_pages = ra_kb >> (PAGE_SHIFT - 10);
+}
+} else
+q->sequential_count = 0;
+q->last_sector = sector;
+q->last_size = bio->bi_iter.bi_size;
+}
 if (blkcg_punt_bio_submit(bio))
 return;
 
diff --git a/block/blk-settings.c b/block/blk-settings.c
index 0046b447268f..c8d4e9f2a1b3 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -19,6 +19,8 @@
 #include "blk-wbt.h"
 #include "blk-throttle.h"
 
+extern unsigned int block_ra_multiplier;
+
 void blk_queue_rq_timeout(struct request_queue *q, unsigned int timeout)
 {
 q->rq_timeout = timeout;
@@ -416,7 +418,18 @@ EXPORT_SYMBOL(blk_queue_alignment_offset);
 void disk_update_readahead(struct gendisk *disk)
 {
 struct request_queue *q = disk->queue;
-
+
+/*
+ * Zen 4 NVMe optimization: Scale readahead with device capabilities
+ * PCIe 5.0 devices can handle much larger readahead windows
+ */
+if (blk_queue_nonrot(q)) {
+/* NVMe/SSD: Larger readahead for sequential workloads */
+blk_queue_io_opt(q, max(queue_io_opt(q), 256U * 1024));
+disk->bdi->ra_pages = max(disk->bdi->ra_pages, 
+  (unsigned long)(512 * block_ra_multiplier));
+}
+
 blk_queue_update_readahead(q);
 }
 EXPORT_SYMBOL_GPL(disk_update_readahead);
-- 
2.43.0
