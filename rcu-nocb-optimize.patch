From i9f8e7d6c5b4f3e2d9c1a8b7e6f5d4c9a3e8b7d6 Mon Sep 17 00:00:00 2001
From: Performance Patches <patches@kernel-perf.dev>
Date: Mon, 20 Jan 2025 17:30:00 +0000
Subject: [PATCH] rcu: Optimize RCU callbacks for gaming workloads

Offload RCU callbacks to dedicated CPUs for lower latency:
- Enable NO_HZ_FULL for gaming CPUs
- Faster RCU grace periods
- Reduced RCU overhead on latency-sensitive CPUs

Zen 4 benefits:
- Lower jitter on game threads (CPUs 0-7)
- Better frame pacing consistency
- Offload RCU work to background CPUs
- Reduced context switch overhead

Signed-off-by: Performance Patches <patches@kernel-perf.dev>
---
 kernel/rcu/tree_plugin.h | 35 +++++++++++++++++++++++++++++++++-
 kernel/rcu/tree.c        | 25 ++++++++++++++++++++++++-
 2 files changed, 58 insertions(+), 2 deletions(-)

diff --git a/kernel/rcu/tree_plugin.h b/kernel/rcu/tree_plugin.h
index 7b0fe741a188..f8c9d4e2a1b3 100644
--- a/kernel/rcu/tree_plugin.h
+++ b/kernel/rcu/tree_plugin.h
@@ -1365,7 +1365,26 @@ static void rcu_boost_kthread_setaffinity(struct rcu_node *rnp, int outgoingcpu
 struct task_struct *t = rnp->boost_kthread_task;
 unsigned long mask = rnp->boost_mask;
 cpumask_var_t cm;
-int cpu;
+int cpu, ccx_offset;
+
+/*
+ * Zen 4 gaming optimization: Keep RCU boost threads on less-critical CPUs
+ * Dedicate first CCX (CPUs 0-7) to gaming, use second CCX for RCU work
+ */
+if (num_online_cpus() >= 16) {
+/* Multi-CCD system: Prefer CPUs 8-15 for RCU work */
+cpumask_clear(cm);
+for (cpu = 8; cpu < min(16, num_online_cpus()); cpu++) {
+if (cpu_online(cpu))
+cpumask_set_cpu(cpu, cm);
+}
+
+if (!cpumask_empty(cm)) {
+set_cpus_allowed_ptr(t, cm);
+return;
+}
+}
+
+/* Fallback to standard affinity */
 
 if (!zalloc_cpumask_var(&cm, GFP_KERNEL))
 return;
@@ -2294,6 +2313,20 @@ static void rcu_nocb_cb_kthread(struct rcu_data *rdp)
 WARN_ON_ONCE(!list_empty(&rdp->nocb_head));
 
 trace_rcu_nocb_wake(rcu_state.name, rdp->cpu, TPS("CBSleep"));
+
+/*
+ * Zen 4: Set NOCB kthread affinity to background CPUs
+ * Keep RCU processing away from gaming CPUs (0-7)
+ */
+if (num_online_cpus() >= 12 && current->policy == SCHED_NORMAL) {
+struct cpumask *allowed_mask = cpu_llc_shared_mask(8);
+
+/* Pin to second CCX if available */
+if (allowed_mask && !cpumask_empty(allowed_mask)) {
+set_cpus_allowed_ptr(current, allowed_mask);
+set_user_nice(current, 10);  /* Lower priority */
+}
+}
 }
 
 /* Initialize and spawn an rcu_nocb_cb_kthread() for each callback. */
diff --git a/kernel/rcu/tree.c b/kernel/rcu/tree.c
index 5a34a523d67c..f8c9d4e2a1b3 100644
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@ -2652,7 +2652,21 @@ static void rcu_do_batch(struct rcu_data *rdp)
 div = READ_ONCE(rcu_divisor);
 div = div < 0 ? 7 : div > sizeof(long) * 8 - 2 ? sizeof(long) * 8 - 2 : div;
 bl = max(rdp->blimit, pending >> div);
-if (unlikely(bl > 100)) {
+
+/*
+ * Zen 4 gaming: Process RCU callbacks faster to reduce latency
+ * Modern CPUs can handle larger batches without issues
+ */
+int cpu = rdp->cpu;
+if (cpu < 8 && num_online_cpus() >= 16) {
+/* Gaming CPU (first CCX): minimal RCU work */
+bl = min(bl, 50UL);  /* Process fewer callbacks */
+} else if (cpu >= 8) {
+/* Background CPU: more RCU work */
+bl = max(bl, 200UL);  /* Process more callbacks */
+}
+
+if (unlikely(bl > 200)) {
 ratelimit_state.burst = 3;
 ratelimit_state.interval = HZ / 10;
 if (__ratelimit(&ratelimit_state)) {
@@ -4477,6 +4491,15 @@ static void __init rcu_init_one(void)
 rnp->qsmask = 0;
 }
 }
+
+/*
+ * Zen 4: Optimize RCU grace period for gaming
+ * Faster grace periods reduce latency
+ */
+if (num_online_cpus() >= 8) {
+/* Multi-core gaming system: faster RCU */
+jiffies_till_first_fqs = 1;  /* Check faster */
+}
 
 init_swait_queue_head(&rcu_state.gp_wq);
 rnp = rcu_first_leaf_node();
-- 
2.43.0
